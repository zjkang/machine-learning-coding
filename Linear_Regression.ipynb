{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwIaZSvJBd/TeXtyLmMhNL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zjkang/machine-learning-coding/blob/main/Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer to github for [linear regression](https://github.com/zjkang/Machine-Learning-Interviews/blob/main/src/MLC/notebooks/linear_regression.ipynb)\n",
        "\n",
        "Formula:\n",
        "* $y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n = X^TW$\n",
        "* loss function: sum of squared errors (SSE) w/ L2 reg\n",
        "  \n",
        "  $\\frac{1}{2N} \\sum(y - \\hat{y})^2 + \\alpha||W||^2$\n",
        "\n",
        "链接中的方法实现了\n",
        "1. x是一维的标量的LR,实现解析解\n",
        "2. x是vector的LR,实现解析解\n",
        "3. x是vector的LR,加入了l2 reg和gradient descent,但是输入X的格式有点奇怪。\n",
        "\n",
        "下面的代码是从3修改,Y = XW\n",
        "1. X = [X1, X2, ..., X_m].T,每一个Xi是一个样本是一个行向量。这样X是一个m*n的矩阵.\n",
        "2. W是n*1的列向量.\n",
        "\n",
        "\n",
        "* Add regularization: Regularization can help prevent overfitting by adding a penalty term to the cost function. One common regularization technique is L2 regularization, which adds the sum of squares of the coefficients to the cost function. This can be easily added to the code by adding a regularization parameter to the constructor.\n",
        "\n",
        "* Use gradient descent: For large datasets, calculating the inverse of the matrix in the normal equation can be computationally expensive. To overcome this, we can use gradient descent to minimize the cost function. This can be implemented by adding a method that updates the coefficients iteratively using the gradient descent algorithm.\n",
        "\n",
        "Note:\n",
        "\n",
        "numpy中如果是一个矩阵和一个向量相乘，可以使用NumPy中的numpy.dot()函数。向量在这里被视为一个列向量，因此可以直接与矩阵相乘。\n",
        "* numpy.dot和@表示的矩阵的乘法\n",
        "* numpy.multiply用作计算内积"
      ],
      "metadata": {
        "id": "O-entkVEixCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class LinearRegressionGD:\n",
        "    def __init__(self, regul=0):\n",
        "        self.regul = regul\n",
        "        self.W = None\n",
        "\n",
        "    def fit(self, X, y, lr=0.01, num_iter=1000):\n",
        "        # Input validation\n",
        "        if len(X) != len(y) or len(X) == 0:\n",
        "            raise ValueError(\"X and y must have the same length and cannot be empty\")\n",
        "\n",
        "        num_samples = len(X)\n",
        "\n",
        "        # Add bias term to X -> [1 X]\n",
        "        X = np.hstack([np.ones((len(X), 1)), X])\n",
        "\n",
        "        # Initialize W to zeros\n",
        "        self.W = np.zeros(X.shape[1])\n",
        "\n",
        "        # Use gradient descent to minimize loss function\n",
        "        for i in range(num_iter):\n",
        "            # Calculate predicted values\n",
        "            y_pred = np.dot(X, self.W)\n",
        "\n",
        "            # Calculate loss function\n",
        "            cost = np.sum((y_pred - y) ** 2) / (2*num_samples) + self.regul * np.sum(self.W ** 2)\n",
        "\n",
        "            # Calculate gradients\n",
        "            dw = np.dot(X.T, (y_pred - y)) / num_samples + 2 * self.regul * self.W\n",
        "\n",
        "            # Update W\n",
        "            self.W = self.W - lr * dw\n",
        "\n",
        "            if (i % 1000 == 0 ): print(f'loss: {cost} with iteration {i}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Add bias term to X\n",
        "        X = np.hstack([np.ones((len(X), 1)), X])\n",
        "\n",
        "        # Calculate predicted values\n",
        "        y_pred = np.dot(X, self.W)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "djt-AO5FuvdU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate true weight and bias for linear regression\n",
        "w_true = np.array([1.0, 2.0])\n",
        "b = 1.0\n",
        "\n",
        "# input X, y\n",
        "X = np.array([[1,2], [2,1], [3,4], [4,2], [5,2]]) # shape(5,2)\n",
        "y = np.dot(X, w_true) + b # shape(5,)\n",
        "print(f'true y = {y}')\n",
        "# y = y + np.random.normal(0, 0.001, y.shape) # add noise\n",
        "print(f'true y with nosie = {y}')\n",
        "\n",
        "\n",
        "lr = LinearRegressionGD(regul=0.001)\n",
        "lr.fit(X, y, lr=0.01, num_iter=10000)\n",
        "print('true weight: ', np.concatenate((np.array([b]), w_true)))\n",
        "print('predicted weight', lr.W)  # Output: [1.03895025 0.92725999 2.05553305]\n",
        "y_pred = lr.predict(X)\n",
        "print('predicted y', y_pred)  # Output: [ 6.07727635  4.94900329 12.04286243  8.85905632  9.78631632]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZEf1UHl3FI_",
        "outputId": "5f5c8af3-521b-4acd-e7b6-e7b5105cab1d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true y = [ 6.  5. 12.  9. 10.]\n",
            "true y with nosie = [ 6.  5. 12.  9. 10.]\n",
            "loss: 38.6 with iteration 0\n",
            "loss: 0.007015458985965686 with iteration 1000\n",
            "loss: 0.0061259592299554 with iteration 2000\n",
            "loss: 0.006011098034525711 with iteration 3000\n",
            "loss: 0.005996265994987784 with iteration 4000\n",
            "loss: 0.005994350731995509 with iteration 5000\n",
            "loss: 0.0059941034138623435 with iteration 6000\n",
            "loss: 0.0059940714776430155 with iteration 7000\n",
            "loss: 0.005994067353715296 with iteration 8000\n",
            "loss: 0.00599406682119203 with iteration 9000\n",
            "true weight:  [1. 1. 2.]\n",
            "predicted weight [0.99213796 1.00197944 1.99997184]\n",
            "predicted y [ 5.99406108  4.99606868 11.99796363  8.9999994  10.00197884]\n"
          ]
        }
      ]
    }
  ]
}