{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFgCjBUTrcqxjYky9EFHvC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zjkang/machine-learning-coding/blob/main/linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[linear regression](https://github.com/zjkang/Machine-Learning-Interviews/blob/main/src/MLC/notebooks/linear_regression.ipynb)\n",
        "\n",
        "loss function: sum of squared errors (SSE)\n",
        "\n",
        "链接中的方法实现了\n",
        "1. x是一维的标量的LR,实现解析解\n",
        "2. x是vector的LR,实现解析解\n",
        "3. x是vector的LR,加入了l2 reg和gradient descent但是输入X的格式有点奇怪。\n",
        "\n",
        "下面的代码是从3修改,Y = W'X, X = [X1, X2, ..., X_m].T,每一个Xi是一个样本。这样X是一个m*n的矩阵.\n",
        "\n",
        "\n",
        "* Add regularization: Regularization can help prevent overfitting by adding a penalty term to the cost function. One common regularization technique is L2 regularization, which adds the sum of squares of the coefficients to the cost function. This can be easily added to the code by adding a regularization parameter to the constructor.\n",
        "\n",
        "* Use gradient descent: For large datasets, calculating the inverse of the matrix in the normal equation can be computationally expensive. To overcome this, we can use gradient descent to minimize the cost function. This can be implemented by adding a method that updates the coefficients iteratively using the gradient descent algorithm.\n",
        "\n",
        "Note:\n",
        "\n",
        "numpy中如果是一个矩阵和一个向量相乘，可以使用NumPy中的numpy.dot()函数。向量在这里被视为一个列向量，因此可以直接与矩阵相乘。\n",
        "* numpy.dot和@表示的矩阵的乘法\n",
        "* numpy.multiply用作计算内积"
      ],
      "metadata": {
        "id": "O-entkVEixCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class LinearRegressionGD:\n",
        "    def __init__(self, regul=0):\n",
        "        self.regul = regul\n",
        "        self.W = None\n",
        "\n",
        "    def fit(self, X, y, lr=0.01, num_iter=1000):\n",
        "        # Input validation\n",
        "        if len(X) != len(y) or len(X) == 0:\n",
        "            raise ValueError(\"X and y must have the same length and cannot be empty\")\n",
        "\n",
        "        # Add bias term to X -> [1 X]\n",
        "        X = np.hstack([np.ones((len(X), 1)), X])\n",
        "\n",
        "        # Initialize W to zeros\n",
        "        self.W = np.zeros(X.shape[1])\n",
        "\n",
        "        # Use gradient descent to minimize loss function\n",
        "        for i in range(num_iter):\n",
        "            # Calculate predicted values\n",
        "            y_pred = np.dot(X, self.W)\n",
        "\n",
        "            # Calculate loss function\n",
        "            cost = np.sum((y_pred - y) ** 2) + self.regul * np.sum(self.W ** 2)\n",
        "\n",
        "            # Calculate gradients\n",
        "            gradients = 2 * np.dot(X.T, (y_pred - y)) + 2 * self.regul * self.W\n",
        "\n",
        "            # Update W\n",
        "            self.W = self.W - lr * gradients\n",
        "\n",
        "            if (i % 1000 == 0 ): print(f'loss: {cost} with iteration {i}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Add bias term to X\n",
        "        X = np.hstack([np.ones((len(X), 1)), X])\n",
        "\n",
        "        # Calculate predicted values\n",
        "        y_pred = np.dot(X, self.W)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "djt-AO5FuvdU"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate true weight and bias for linear regression\n",
        "w_true = np.array([1.0, 2.0])\n",
        "b = 1.0\n",
        "\n",
        "# input X, y\n",
        "X = np.array([[1,2], [2,1], [3,4], [4,2], [5,2]]) # shape(5,2)\n",
        "y = np.dot(X, w_true) + b # shape(5,)\n",
        "print(f'true y = {y}')\n",
        "y = y + np.random.normal(0, 0.01, y.shape) # add noise\n",
        "print(f'true y with nosie = {y}')\n",
        "\n",
        "\n",
        "lr = LinearRegressionGD(regul=0.1)\n",
        "lr.fit(X, y, lr=0.01, num_iter=10000)\n",
        "print('true weight: ', np.concatenate((np.array([b]), w_true)))\n",
        "print('predicted weight', lr.W)  # Output: [1.03895025 0.92725999 2.05553305]\n",
        "y_pred = lr.predict(X)\n",
        "print('predicted y', y_pred)  # Output: [ 6.07727635  4.94900329 12.04286243  8.85905632  9.78631632]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZEf1UHl3FI_",
        "outputId": "f1dd02f0-c6bb-4413-db54-357ac87a6249"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true y = [ 6.  5. 12.  9. 10.]\n",
            "true y with nosie = [ 6.00920736  4.99844841 12.02666574  8.99147819  9.99314865]\n",
            "cost: 386.44544835555865 with iteration 0\n",
            "cost: 0.5970290063635355 with iteration 1000\n",
            "cost: 0.5970290063615518 with iteration 2000\n",
            "cost: 0.5970290063615518 with iteration 3000\n",
            "cost: 0.5970290063615518 with iteration 4000\n",
            "cost: 0.5970290063615518 with iteration 5000\n",
            "cost: 0.5970290063615518 with iteration 6000\n",
            "cost: 0.5970290063615518 with iteration 7000\n",
            "cost: 0.5970290063615518 with iteration 8000\n",
            "cost: 0.5970290063615518 with iteration 9000\n",
            "true weight:  [1. 1. 2.]\n",
            "predicted weight [0.92750716 1.01292557 2.0086162 ]\n",
            "predicted y [ 5.95766514  4.96197451 12.00074869  8.99644186 10.00936743]\n"
          ]
        }
      ]
    }
  ]
}